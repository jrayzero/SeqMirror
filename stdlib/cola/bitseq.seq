from C import cola_bitstr_uint(u64, int) -> str

# TODO really need to double check that my integer sizes/bit ops are correct cause I'm not great with this part
u1 = UInt[1]

# basic idea: the individual values you append (or init with) have their MSB bits trimmed off to match the specified size
# then that value is shifted toward the MSB of the current bitseq. So, bits are always packed from MSB->LSB.
# bit_seqs[0] -> MSB 
# bit_seqs[-1] -> LSB
# Thus, bitwidths for appending measure from LSB to MSB, but effective width measures from MSB->LSB
class BitSeq:

  # for storage, everythign is aligned towards the LSB. But when you pack, we align to the MSB, pack after the MSB (towards
  # the LSB), and then realign to the LSB
  effective_width: int
  bit_seqs: list[u64]
  nseqs: int

  # completely brain-dead and slow bit packing that just iterates through to pack one bit at a time
  def _pack_bits(self, val: u64, width: int):
    it = iter(val)
    # skip the MSBs we don't need
    for _ in range(64-width):      
      next(it)
    for i in range(width):
      v = u64(next(it))
      cur_bit_seq = u64(0)
      if self.nseqs == 0 or self.effective_width & 63 == 0:
        # need a new sequence
        self.bit_seqs.append(cur_bit_seq)
        self.nseqs += 1
      else:
        # MSB align the current bitseq
        align_shift = 64 - self.effective_width & 63
        # sanity check
        assert align_shift != 64 # should never reach this
        cur_bit_seq = self.bit_seqs[-1] << u64(align_shift)
      msb_bit_idx = self.effective_width & 63 # counted from the MSB->LSB
      v <<= u64(63-msb_bit_idx)
      cur_bit_seq |= v
      # LSB align again
      self.effective_width += 1
      rshift = 64 - self.effective_width & 63
      self.bit_seqs[-1] = (cur_bit_seq >> u64(rshift))

  # width here is measured from the LSB->MSB, which is different than the effective_width which measures
  # from MSB->LSB. I do this because if you want to pass in decimal values (say 10, which is 0b1010, or maybe 0b000001010),
  # you wouldn't want to take the MSB bits in this case.

  def __init__(self):
    self.effective_width = 0
    self.bit_seqs = []
    self.nseqs = 0

  # width can be different than the actual type since you might not want all of the bits
  def __init__(self, val: u1):
    self.__init__()
    self._pack_bits(u64(val), 1)

  def __init__(self, val: u8, width: int=8):
    self.__init__()
    assert width >= 1 and width <= 8
    self._pack_bits(u64(val), width)

  def __init__(self, val: u16, width: int=16):
    self.__init__()
    assert width >= 1 and width <= 16
    self._pack_bits(u64(val), width)

  def __init__(self, val: u32, width: int=32):
    self.__init__()
    assert width >= 1 and width <= 32
    self._pack_bits(u64(val), width)

  def __init__(self, val: u64, width: int=64):
    self.__init__()
    assert width >= 1 and width <= 64
    self._pack_bits(u64(val), width)

  def __init__(self, val: int, width: int=64): 
    self.__init__()
    assert width >= 1 and width <= 64
    self._pack_bits(u64(val), width)

  def __init__(self, s: str, width: int):
    self.__init__()
    assert width >= 1 and width <= 64
    assert 8 * len(s) >= width # need enough bits
    sum = u64(0)   
    # go through each bit. This is so we can handle odd numbers of bits
    for idx in range(width):
      cur_byte = idx // 8
      # get the bit we are intersted in
      bitidx = idx & 7
      bit = u1(u8(s.ptr[cur_byte]) >> u8(bitidx))
      bit &= u1(1)
      shft = u64(cur_byte * 8 + (idx & 7))
      shftbit = u64(bit) << shft
      sum |= shftbit
    self._pack_bits(sum, width)

  def nbits(self) -> int:
    return self.effective_width

  def pack(self, val: BitSeq):
    w = val.effective_width
    for bitseq in val.bit_seqs[:-1]:
      self._pack_bits(bitseq, 64)
      w -= 64
    # need to align the last one to the right hand side
    last_seq = val.bit_seqs[-1]
#    last_seq >>= u64(64-w)
    self._pack_bits(last_seq, w)

  def pack(self, val: u1):
    self._pack_bits(u64(val), 1)

  def pack(self, val: u8, width: int=8):
    assert width >= 1 and width <= 8
    self._pack_bits(u64(val), width)

  def pack(self, val: u16, width: int=16):
    assert width >= 1 and width <= 16
    self._pack_bits(u64(val), width)

  def pack(self, val: u32, width: int=32):
    assert width >= 1 and width <= 32
    self._pack_bits(u64(val), width)

  def pack(self, val: u64, width: int=64):
    assert width >= 1 and width <= 64
    self._pack_bits(u64(val), width)

  def pack(self, val: int, width: int=64): 
    assert width >= 1 and width <= 64
    self._pack_bits(u64(val), width)

  def __iter__(self) -> Generator[UInt[1]]:
    for i in range(self.nseqs-1):      
      for b in self.bit_seqs[i]:
        yield UInt[1](b)
    w = self.effective_width & 63
    it = iter(self.bit_seqs[self.nseqs-1])
    for i in range(int(w)):
      yield next(it)
    it.destroy()

  # prints out in binary
  def __str__(self) -> str:
    s = ''
    w = self.effective_width
    for i in range(self.nseqs-1):
      s += self.bit_seqs[i].to_bitstr(64)    
      w -= 64
    s += self.bit_seqs[-1].to_bitstr(w)
    return '0b' + s

  def __eq__(self, other: BitSeq) -> bool:
    if self.effective_width != other.effective_width:
      return False
    w = self.effective_width
    for i in range(self.nseqs-1):
      self_bs = self.bit_seqs[i]
      other_bs = other.bit_seqs[i]
      if self_bs != other_bs:
        return False
      w -= 64
    # remaining MSB bits
    # get last bitseq and mask off anything on the LSB (rhs) side that isn't needed
    self_bs = self.bit_seqs[self.nseqs-1] & u64(~((1<<(64-w))-1))
    other_bs = other.bit_seqs[self.nseqs-1] & u64(~((1<<(64-w))-1))
    if self_bs != other_bs:
      return False
    return True
      
  def __ne__(self, other: BitSeq) -> bool:
    return not (self == other)

  def __int__(self) -> int:
    if self.nseqs == 0:
      return 0
    else:
      return int(self.bit_seqs[0])

# used in conjunction with int/uint[N].new to get stuff as raw unisnged chars
# get raw unsigned characters from a string
@extend
class byte:
  def zext_or_trunc[T: int](self) -> UInt[T]:
    if 8 < T:
      return UInt.uint_zext[8,T](self)
    elif 8 == T:
      return UInt.ident[8](self)
    else: # N > T
      return UInt.uint_trunc[8,T](self)    

@extend 
class UInt[N]:

  def __new__(val: BitSeq) -> UInt[N]:
    ui = UInt[N]()
    idx = 0
    # go through the fully filled bitseqs
    for bs in val.bit_seqs[:-1]:
      ui |= UInt[N](bs) << UInt[N](N-idx*N)
      idx += 1
    bs = val.bit_seqs[-1]
    bits = UInt[N](bs) << UInt[N](N - val.effective_width & (N-1))#UInt[N]((val.nseqs-1)*N)
    ui |= bits
    # LSB align the whole thing nwo
    ui >>= UInt[N](N - val.effective_width & (N-1))
    return ui

  def __new__(val) -> UInt[N]:
    return val.zext_or_trunc[N]()
  @llvm
  def ident[T: int](what) -> UInt[T]:
    ret i{=T} %what
  @llvm
  def uint_zext[F: int, T: int](what) -> UInt[T]:
    %0 = zext i{=F} %what to i{=T}
    ret i{=T} %0
  @llvm
  def uint_trunc[F: int, T: int](what) -> UInt[T]:
    %0 = trunc i{=F} %what to i{=T}
    ret i{=T} %0
  def zext_or_trunc[T: int](self) -> UInt[T]:
    if N < T:
      return UInt.uint_zext[N,T](self)
    elif N == T:
      return self
    else: # N > T
      return UInt.uint_trunc[N,T](self)

  # this is MSB to LSB iter
  def __iter__(self) -> Generator[u1]:
    for i in range(N):
      yield u1((self >> u64(64-i-1)) & u64(1))
      
  # this is LSB to MSB iter
  def inverse_iter(self) -> Generator[u1]:
    for i in range(N):
      yield u1(self >> u64(i))

  # this is MSB to LSB stringify
  def to_bitstr(self, bitwidth: int) -> str:
    return cola_bitstr_uint(u64(self), bitwidth)

@extend
class File:
  def write(self, arr: Array[byte]):
    self._ensure_open() 
    _C.fwrite(arr.ptr, 1, len(arr), self.fp) 
    self._ensure_open() 
